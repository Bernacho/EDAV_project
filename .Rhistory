mutate(character = if_else(character == "PHOE" ,"PHOEBE", character))
head(dialogues)
dialogues <- dialogues %>%
mutate(words = if_else(is.na(line) , 1, as.double(words))) %>%
mutate(line = if_else(is.na(line) ,"-", line))
main_characters <- c("MONICA","RACHEL","PHOEBE","ROSS","CHANDLER","JOEY")
dialogues %>%
filter(character %in% main_characters) %>%
ggplot(aes(fct_infreq(character))) +
geom_bar(fill="#0d1fe6") +
ggtitle("Main characters lines count") +
labs(x="character",y="lines count") +
theme_bw()
dialogues_by_episode <- dialogues %>%
group_by(episode_id) %>%
summarise(lines=paste(line, collapse = " "))
head(dialogues_by_episode)
url <- "https://fangj.github.io/friends/"
paths_allowed(url)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(rvest)
library(robotstxt)
library(tidyverse)
library(textmineR)
library(extracat)
url <- "https://fangj.github.io/friends/"
paths_allowed(url)
html <- read_html(url)
episodes_list <- html %>% html_nodes("li")  %>% html_nodes("a")
link <- episodes_list %>% html_attr("href")
long_name <- episodes_list %>% html_text
df <- data.frame(long_name,link) %>%
mutate(numbers = str_extract(long_name,"(\\d+-\\d+)|\\d+") ) %>%
mutate(season=ifelse(str_length(str_match(numbers,"\\d+"))==3, substr(numbers,0,1),substr(numbers,0,2) ) ) %>%
mutate(number=str_replace(str_replace(numbers,season,""), paste("-",season,sep = "")  ,"-") ) %>%
mutate(name = trimws(str_replace(long_name,"(\\d+-\\d+)|\\d+","")) )%>%
mutate(link = paste(url,link,sep="")) %>%
mutate(id=paste(season,":",number))
df <- df[colnames(df)!="numbers"]
read_dialogues <- function(link,ep_id) {
html <- read_html(link)
#### Special case for episode 9 : 15
if (ep_id == "9 : 15"){
df_1 <- read.csv("data/episode_9_15.csv")
df_2 <- df_1 %>%
mutate(rows2= trimws(str_replace(rows,"\\(([^\\)]+)\\)"," ") )) %>%
mutate(is_scene = ifelse(substr(rows2,0,1)=="[",1,0))
}else{
if(length(html %>% html_nodes("p,font p"))<10 ){
rows <- html %>% html_nodes("font[size='3']") %>%
gsub(pattern = '<.*?>', replacement = "|")
if(length(rows)==0){
rows <- html %>%
gsub(pattern = '<br\\s*/?>', replacement = "|")
}
df_1 <-data.frame()
for (i in 1:length(rows)) {
rows_2 <- strsplit(rows[i],split="[|]")
df_temp <- data.frame(rows_2)
names(df_temp) <- c("rows")
df_1 <- rbind(df_1, df_temp)
}
df_2 <- df_1 %>%
mutate(rows2= trimws(rows)) %>%
mutate(rows2 = str_replace_all(rows2, "[\n]" , " ")) %>%
mutate(rows2 = str_replace_all(rows2, "<b>" , " ")) %>%
mutate(rows2 = str_replace_all(rows2, "</b>" , " ")) %>%
mutate(is_scene = ifelse(substr(rows2,0,1)=="[" | substr(rows2,0,1)=="(",1,0)) %>%
mutate(rows2= trimws(ifelse(is_scene==1,rows2,str_replace(rows2,"\\[([^\\)]+)\\]"," "))   ))
}else{
rows <- html %>% html_nodes("p,font p") %>% html_text()
df_1 <- data.frame(rows)
df_2 <- df_1 %>%
mutate(rows2= trimws(str_replace(rows,"\\(([^\\)]+)\\)"," ") )) %>%
mutate(is_scene = ifelse(substr(rows2,0,1)=="[",1,0))
}
}
df_2$split <- str_locate(df_2[,"rows2"],  "\\w+:|\\w+.\\w+:|\\w+.\\s\\w+:" )[,2]
df_3 <- df_2 %>%
mutate(character=ifelse(is.na(split),"",toupper(trimws(substr(rows2,0,split - 1 ))))) %>%
mutate(line=ifelse(is.na(split),"",trimws(substr(rows2,split + 1,1000000)))) %>%
mutate(episode_id = ep_id) %>%
mutate(line=trimws(str_replace_all(line,"\\(([^\\)]+)\\)"," ")) ) %>%
mutate(line=trimws(str_replace_all(line,"[\r\n]"," ")) ) %>%
mutate(line=trimws(str_replace_all(line,'[\"]'," ")) ) %>%
mutate(line=trimws(str_replace_all(line,"\\s+"," ")) ) %>%
filter(character != "WRITTEN BY")
df_4 <- data.frame(df_3, scene=cumsum(df_3$is_scene))
df_5 <- df_4 %>%
filter(rows2!="",!is_scene,!is.na(character),character!="")
df_5$line_num <- seq.int(nrow(df_5))
df_6<- df_5 %>%
dplyr::select(c("episode_id","line_num","scene",'character','line'))
return(df_6)
}
dialogues <- data.frame()
for (i in 1:nrow(df)) {
n <- read_dialogues(df[i,"link"],df[i,"id"])
# print(i)
# print(nrow(n))
dialogues <- rbind(dialogues, n)
}
head(dialogues)
write_csv(dialogues, "data/friends_dialogues.csv")
write_csv(dialogues, "data/friends_scraping.csv")
# To speed up the process we just read the data file instead of scraping
dialogues <- read_csv("data/friends_scraping.csv")
head(dialogues)
dialogues <- dialogues %>%
mutate(words = sapply(strsplit(line, " "), length))
head(dialogues %>% select(line,words))
unique(filter(dialogues, grepl("-",episode_id))$episode_id)
dialogues <- dialogues %>%
mutate(episode_id = if_else(episode_id == "2 : 12-13" & scene < 36,"2 : 12",episode_id)) %>%
mutate(episode_id = if_else(episode_id == "2 : 12-13" & scene >= 36,"2 : 13",episode_id)) %>%
mutate(episode_id = if_else(episode_id == "6 : 15-16" & scene < 15,"6 : 15",episode_id)) %>%
mutate(episode_id = if_else(episode_id == "6 : 15-16" & scene >= 15,"6 : 16",episode_id)) %>%
mutate(episode_id = if_else(episode_id == "9 : 23-24" & scene < 16,"9 : 23",episode_id)) %>%
mutate(episode_id = if_else(episode_id == "9 : 23-24" & scene >= 16,"9 : 24",episode_id)) %>%
mutate(episode_id = if_else(episode_id == "10 : 17-18" ,"10 : 17",episode_id))
dialogues <- dialogues %>%
mutate(season =  trimws(str_extract(episode_id,".+\\:"))) %>%
mutate(episode =trimws(str_extract(episode_id,"\\:.+")))
dialogues <- dialogues %>%
mutate(season =  as.integer( trimws(str_sub(season, end=-2)))) %>%
mutate(episode = as.integer(trimws(str_sub(episode, start=2))))
dialogues <- dialogues %>%
mutate(character = if_else(character == "RACH" ,"RACHEL", character)) %>%
mutate(character = if_else(character == "MNCA" ,"MONICA", character)) %>%
mutate(character = if_else(character == "CHAN" ,"CHANDLER", character)) %>%
mutate(character = if_else(character == "PHOE" ,"PHOEBE", character))
head(dialogues)
colSums(is.na(dialogues))
dialogues <- dialogues %>%
mutate(words = if_else(is.na(line) , 1, as.double(words))) %>%
mutate(line = if_else(is.na(line) ,"-", line))
write_csv(dialogues, "data/friends_dialogues.csv")
main_characters <- c("MONICA","RACHEL","PHOEBE","ROSS","CHANDLER","JOEY")
dialogues %>%
filter(character %in% main_characters) %>%
ggplot(aes(fct_infreq(character))) +
geom_bar(fill="#0d1fe6") +
ggtitle("Main characters lines count") +
labs(x="character",y="lines count") +
theme_bw()
dialogues_by_episode <- dialogues %>%
group_by(episode_id) %>%
summarise(lines=paste(line, collapse = " "))
head(dialogues_by_episode)
dialogues_by_episode <- dialogues %>%
group_by(episode_id) %>%
summarise(lines=trim(paste(line, collapse = " ")))
head(dialogues_by_episode)
dialogues_by_episode <- dialogues %>%
group_by(episode_id) %>%
summarise(lines=as.character(trim(paste(line, collapse = " "))))
head(dialogues_by_episode)
dialogues_by_episode <- dialogues %>%
group_by(episode_id) %>%
summarise(lines=noquote(trim(paste(line, collapse = " "))))
head(dialogues_by_episode)
dialogues_by_episode <- dialogues %>%
group_by(episode_id) %>%
summarise(lines=trim(paste(line, collapse = " ")))
head(dialogues_by_episode)
#create DTM
dtm <- CreateDtm(dialogues_by_episode$lines,
doc_names = dialogues_by_episode$episode_id,
stopword_vec = c(stopwords::stopwords("en"),
stopwords::stopwords(source = "smart"),
tolower(main_characters),
c("yeah","uh","ah","ya","umm","ow","hey","um","huh",
"uhm","gonna","wanna","ohh","ooh","mr","ahh","whoa",
"la","ha")),
ngram_window = c(1, 2))
dtm <- dtm[,colSums(dtm) > 2]
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
head(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the documents
vocabulary <- tf[ tf$term_freq > 1 && tf$doc_freq < nrow(dtm) / 2 ]
set.seed(12345)
model <- FitLdaModel(dtm = dtm,
k = 15,
iterations = 1000,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE,
cpus = 2)
summary(model$coherence)
hist(model$coherence,
col= "blue",
main = "Histogram of probabilistic coherence")
# Get the top terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 5)
print(t(model$top_terms))
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
# prevalence should be proportional to alpha
plot(model$prevalence, model$alpha, xlab = "prevalence", ylab = "alpha")
# put them together, with coherence into a summary table
model$summary <- data.frame(topic = rownames(model$phi),
coherence = round(model$coherence, 3),
prevalence = round(model$prevalence,3),
top_terms = apply(model$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
model$summary[ order(model$summary$prevalence, decreasing = TRUE) , ]
#create DTM
dtm <- CreateDtm(dialogues_by_episode$lines,
doc_names = dialogues_by_episode$episode_id,
stopword_vec = c(stopwords::stopwords("en"),
stopwords::stopwords(source = "smart"),
tolower(main_characters),
c("yeah","uh","ah","ya","umm","ow","hey","um","huh",
"uhm","gonna","wanna","ohh","ooh","mr","ahh","whoa",
"la","ha","ugh")),
ngram_window = c(1, 2))
dtm <- dtm[,colSums(dtm) > 2]
#create DTM
set.seed(12345)
dtm <- CreateDtm(dialogues_by_episode$lines,
doc_names = dialogues_by_episode$episode_id,
stopword_vec = c(stopwords::stopwords("en"),
stopwords::stopwords(source = "smart"),
tolower(main_characters),
c("yeah","uh","ah","ya","umm","ow","hey","um","huh",
"uhm","gonna","wanna","ohh","ooh","mr","ahh","whoa",
"la","ha","ugh")),
ngram_window = c(1, 2))
dtm <- dtm[,colSums(dtm) > 2]
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
head(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the documents
vocabulary <- tf[ tf$term_freq > 1 && tf$doc_freq < nrow(dtm) / 2 ]
set.seed(12345)
model <- FitLdaModel(dtm = dtm,
k = 15,
iterations = 1000,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE,
cpus = 2)
summary(model$coherence)
hist(model$coherence,
col= "blue",
main = "Histogram of probabilistic coherence")
# Get the top terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 5)
print(t(model$top_terms))
# Get the top terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 5)
print(t(model$top_terms))
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
# prevalence should be proportional to alpha
plot(model$prevalence, model$alpha, xlab = "prevalence", ylab = "alpha")
# put them together, with coherence into a summary table
model$summary <- data.frame(topic = rownames(model$phi),
coherence = round(model$coherence, 3),
prevalence = round(model$prevalence,3),
top_terms = apply(model$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
model$summary[ order(model$summary$prevalence, decreasing = TRUE) , ]
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2)
write.csv(assignments,"data/topics_assignments.csv")
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2)
assignments2 <- merge(x = assignments, y = model$summary[,c("topic","top_terms")], by = "topic", all.x = TRUE)
#write.csv(assignments,"data/topics_assignments.csv")
View(assignments2)
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2)
assignments2 <- merge(x = assignments, y = model$summary[,c("topic","top_terms")], by = "topic", all.x = TRUE)
write.csv(assignments2,"data/topics_assignments.csv")
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2) %>%
mutate(topic=  as.integer(substr(topic,2,1000000)) )
assignments2 <- merge(x = assignments, y = model$summary[,c("topic","top_terms")], by = "topic", all.x = TRUE)
write.csv(assignments2,"data/topics_assignments.csv")
View(assignments2)
dialogues_by_episode <- dialogues %>%
group_by(episode_id) %>%
summarise(lines=trim(paste(line, collapse = " ")))
head(dialogues_by_episode)
#create DTM
set.seed(12345)
dtm <- CreateDtm(dialogues_by_episode$lines,
doc_names = dialogues_by_episode$episode_id,
stopword_vec = c(stopwords::stopwords("en"),
stopwords::stopwords(source = "smart"),
tolower(main_characters),
c("yeah","uh","ah","ya","umm","ow","hey","um","huh",
"uhm","gonna","wanna","ohh","ooh","mr","ahh","whoa",
"la","ha","ugh")),
ngram_window = c(1, 2))
dtm <- dtm[,colSums(dtm) > 2]
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
head(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the documents
vocabulary <- tf[ tf$term_freq > 1 && tf$doc_freq < nrow(dtm) / 2 ]
set.seed(12345)
model <- FitLdaModel(dtm = dtm,
k = 15,
iterations = 1000,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE,
cpus = 2)
summary(model$coherence)
hist(model$coherence,
col= "blue",
main = "Histogram of probabilistic coherence")
# Get the top terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 5)
print(t(model$top_terms))
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
# prevalence should be proportional to alpha
plot(model$prevalence, model$alpha, xlab = "prevalence", ylab = "alpha")
# put them together, with coherence into a summary table
model$summary <- data.frame(topic = rownames(model$phi),
coherence = round(model$coherence, 3),
prevalence = round(model$prevalence,3),
top_terms = apply(model$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
model$summary[ order(model$summary$prevalence, decreasing = TRUE) , ]
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2) %>%
mutate(topic=  as.integer(substr(topic,2,1000000)) )
assignments2 <- merge(x = assignments, y = model$summary[,c("topic","top_terms")], by = "topic", all.x = TRUE)
write.csv(assignments2,"data/topics_assignments.csv")
head(assignments)
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2) %>%
mutate(topic_num=  as.integer(substr(topic,1,1000000)) )
head(assignments)
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2) %>%
mutate(topic_num=  substr(topic,2,1000000) )
head(assignments)
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2) %>%
mutate(topic_num=  as.integer(substr(topic,3,1000000)) )
head(assignments)
assignments <- rownames_to_column(as.data.frame(t(model$theta)),var="rowID") %>%
gather(key="topic",value="value",-rowID) %>%
rename(topic2=topic) %>%
rename(rowID2=rowID) %>%
rename(topic=rowID2) %>%
rename(rowID=topic2) %>%
mutate(topic_num=  as.integer(substr(topic,3,1000000)) )
assignments2 <- merge(x = assignments, y = model$summary[,c("topic","top_terms")], by = "topic", all.x = TRUE)
write.csv(assignments2,"data/topics_assignments.csv")
stopwords::stopwords(source = "smart")
head(original_tf[order(tf), decreasing=TRUE])
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
head(original_tf[order(tf), decreasing=TRUE])
View(tf)
head(original_tf[order(term_freq), decreasing=TRUE])
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
head(original_tf[order(original_tf$term_freq), decreasing=TRUE])
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
head( original_tf[ order(original_tf$term_freq, decreasing=TRUE), ] )
summary(model$coherence)
hist(model$coherence,
col= "blue",
main = "Histogram of probabilistic coherence")
summary(model$coherence)
hist(model$coherence,
col= "#0d1fe6",
main = "Histogram of probabilistic coherence")
summary(model$coherence)
hist(model$coherence,
col= "#0d1fe6",
main = "Histogram of probabilistic coherence")
model$coherence
summary(model$coherence)
hist(model$coherence,
col= "#0d1fe6",
main = "Histogram of probabilistic coherence")
ggplot(model$coherence, aes(x=model$coherence,y=..density..)) +
geom_histogram(bins=40, color = "#0d1fe6", fill = "lightblue", boundary = 0) +
theme_bw()
summary(model$coherence)
hist(model$coherence,
col= "#0d1fe6",
main = "Histogram of probabilistic coherence")
ggplot(model, aes(x=coherence,y=..density..)) +
geom_histogram(bins=40, color = "#0d1fe6", fill = "lightblue", boundary = 0) +
theme_bw()
summary(model$coherence)
hist(model$coherence,
col= "#0d1fe6",
main = "Histogram of topic coherence")
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
ggplot(t(model)) +
geom_point(aes(x=prevalence,y=alpha))
class(model)
class(t(model))
class(model$prevalence)
ggplot() +
geom_point(aes(x=model$prevalence,y=model$alpha))
summary(model$coherence)
ggplot(aes(x=model$coherence,y=..density..)) +
geom_histogram(bins=40, color = "#0d1fe6", fill = "lightblue", boundary = 0) +
geom_density(color = "#bd02da") +
theme_bw()
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
ggplot() +
geom_point(aes(x=model$prevalence,y=model$alpha), color = "#bd02da")+
theme_bw()
ggplot() +
geom_point(aes(x=model$prevalence,y=model$alpha), color = "#0d1fe6")+
ggtitle("Topic prevalence") +
labs(x="prevalence",y="alpha") +
theme_bw()
# put them together, with coherence into a summary table
model$summary <- data.frame(topic = rownames(model$phi),
coherence = round(model$coherence, 3),
prevalence = round(model$prevalence,3),
top_terms = apply(model$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
model$summary <- data.frame(topic = rownames(model$phi),
coherence = round(model$coherence, 3),
prevalence = round(model$prevalence,3),
top_terms = apply(model$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
model$summary[ order(model$summary$prevalence, decreasing = TRUE) , ]
head(assignments2)
head(model$theta[,1:5])
head(model$phi[,1:5])
head(model$phi[,0:5])
head(model$phi[,0:5])
head(model$phi[,:5])
head(model$phi[, :5])
1:5
0:5
