---
editor_options:
  chunk_output_type: console
output:
  html_document: default
---

<style type="text/css">
.main-container {
  max-width: 1400px;
  margin-left: auto;
  margin-right: auto;
}

body{ /* Normal  */
      font-size: 18px;
  }
  
/* Headers */
h2,h5{
  font-size: 30pt;}
  
  code.r{
  font-size: 14px;
}

pre {
  font-size: 14px
}
  
</style>



<p style="text-align: center;"><a href="https://fontmeme.com/friends-tv-series-font/"><img src="https://fontmeme.com/permalink/191211/c38580a1d391d18ad5079b34977bb940.png" alt="friends-tv-series-font" border="0"></a></p>

<p style="text-align: center;"><a href="https://fontmeme.com/friends-tv-series-font/"><img src="https://fontmeme.com/permalink/191211/f0004919fd179262830b98cd6e18e52b.png" alt="friends-tv-series-font" border="0"></a></p>


\
\


<p style="text-align: center;"><a href="https://fontmeme.com/friends-tv-series-font/"><img src="https://fontmeme.com/permalink/191211/f7d3118720f664779fee95240eaf0c16.png" alt="friends-tv-series-font" border="0"></a></p>


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(rvest)
library(robotstxt)
library(tidyverse)
library(Hmisc)
library(textmineR)
library(extracat)
library(ggrepel)
library(cluster)
library(igraph, quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library(visNetwork)
library(shiny)
library(ggplot2)
library(Cairo)

```

\
<center>
![](https://media.giphy.com/media/yrsaQdz7sVems/giphy.gif)
</center>

## **I. Introduction**

### ***i) Friends iconic sitcom***

**Friends** is an American situation comedy, created by David Crane and Marta Kauffman, which aired on NBC from September 22, 1994, to May 6, 2004, lasting ten seasons. With an ensemble cast starring **Jennifer Aniston (Rachel)**, **Courteney Cox (Monica)**, **Lisa Kudrow (Phoebe)**, **Matt LeBlanc (Joey)**, **Matthew Perry (Chandler)** and **David Schwimmer (Ross)**.


The show revolved around six friends in their 20s and 30s who lived in Manhattan, New York City. Rachel Green, a sheltered but friendly woman, flees her wedding day and her rich yet unfulfilling life, and finds childhood friend Monica Geller, a tightly-wound but caring chef. After Rachel becomes a waitress at coffee house Central Perk, she and Monica become roommates at Monica's apartment located directly above Central Perk, and Rachel joins Monica's group of single people in their mid-20s: her previous roommate Phoebe Buffay, an eccentric, innocent masseuse; her neighbor across the hall Joey Tribbiani, a dim-witted yet loyal struggling actor and womanizer; Joey's roommate Chandler Bing, a sarcastic, self-deprecating IT manager; and her older brother and Chandler's college roommate Ross Geller, a sweet-natured but insecure paleontologist.



Friends received positive reviews throughout its run and became one of the most popular sitcoms of its time. The series won many awards and was nominated for 63 Primetime Emmy Awards. The series was also very successful in the ratings, consistently ranking in the top ten in the final primetime ratings. Friends has made a large cultural impact, and has become an the model to follow for sitcoms.

### ***ii) Motivation & Questions***

As teenagers at the beginning of the century, we were heavily influenced by the Friends phenomenon and became huge fans of the sitcom. We decided to work on this project to challenge through a data analysis our preconceptions of the show and discover hiden insights. The questions that guide our quantitative assessment are the following:

* Can we categorize by importance all the appearing characters of the sitcom? This question at first glance could seem simple but under the assumption that we do not possess any previous knowledge of the sitcom and considering that over the ten seasons more than 800 characters appeared in the show, the analysis represents a challenge.

* Can we identify and quantify the interactions between the main and secondary characters? What would be an appropriate way to quantify and visualize these relationships? 

* Which are the most recurrent topics through the seasons and episodes of the show? And how the thematic of the show evolved over its ten seasons? Can we extract this information through the dialogues of the show?

* Can we determine the contribution of each character to the popularity of the sitcom? Does the participation of each character influence the viewer's preferences?

### ***iii) R Libraries, Machine Learning techniques & Other resources***

We have use the next R libraries for the development of this project :

* For data extraction and manipulation: [`dplyr`](https://www.rdocumentation.org/packages/dplyr){target="_blank"}, [`rvest`](https://www.rdocumentation.org/packages/rvest/versions/0.3.5){target="_blank"}, [`robotstxt`](https://www.rdocumentation.org/packages/robotstxt/versions/0.6.2){target="_blank"},[`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2){target="_blank"}, [`Hmisc`](https://www.rdocumentation.org/packages/Hmisc/versions/4.3-0){target="_blank"}


* For data visualization: [`ggplot2`](https://www.rdocumentation.org/packages/ggplot2/versions/3.2.1){target="_blank"}, [`ggthemes`](https://www.rdocumentation.org/packages/ggthemes/versions/3.5.0){target="_blank"}, [`ggrepel`](https://www.rdocumentation.org/packages/ggrepel/versions/0.8.1){target="_blank"}, [`visNetwork`](https://www.rdocumentation.org/packages/visNetwork/versions/2.0.9){target="_blank"}, [`d3`](https://d3js.org/){target="_blank"}, [`shiny`](https://www.rdocumentation.org/packages/shiny/versions/1.4.0){target="_blank"}

* Furthermore, for run some of the analysis and visualizations we have used some Machine Learning (ML) techniques and other statistical tools such as: Kmeans analysis([`cluster`](https://www.rdocumentation.org/packages/cluster/versions/2.1.0){target="_blank"}), Graph and network analysis([`igraph`](https://www.rdocumentation.org/packages/igraph/versions/1.2.4.2){target="_blank"}), Topic analysis([`textmineR`](https://www.rdocumentation.org/packages/textmineR/versions/3.0.4){target="_blank"}, [`stopwords`](https://www.rdocumentation.org/packages/stopwords/versions/1.0){target="_blank"})
 

## **II. Data sources**

### ***i) Primary sources***

The primary data sources that we used for our project and that we consider that have an adequate quality are:

   * Transcripts: For the transcripts, we used an open resource built by fans of the sitcom and that has been compiled in a Github repository. The repository contains all the dialogues of the characters for the 231 episodes of the tv-show. The data is organized in Html documents.. The data can be accessed via: https://fangj.github.io/friends/. If you want to see how de transcripts are originally presented [please click here](https://fangj.github.io/friends/season/0101.html).

   * Ratings: For the ratings, we have used the IMDb Datasets which is available for access to customers for personal and non-commercial use.  The data is structured in seven compressed CSV files that contain general information of the show (genre, start year, end year, episode duration, etc.), and specific information of each episode (title, rating, characters, crew, etc.). A relevant characteristic of the database is that it is refreshed daily. We have made the consultation of the Data on November 10, 20199. The data can be accessed via: https://datasets.imdbws.com/


### ***ii) Data quality and challenges***

   * IMDb Dataset: 
   
      + The first obstacle that we faced with the IMDb datasets was the size of the data sets, some of them have millions of rows with the information of Tv-series, shorts, movies, documentaries, and other entertainment formats. It was impossible to store them in our Github.
            
      + The second obstacle was to track, which was the data corresponding to our case of study. For example, we searched in the dataset only by name 'Friends' we found 178 results of TV-series or movies called 'Friends'. It was necessary to understand and do some research on the years of beginning and end of the series to refine the search.
      
      +  Another obstacle was that the ID for TV-series across the seven IMDb datasets was not uniform. For example, in the dataset corresponding to the titles of the TV-series, the ID to identify the show is named "tconst", while on the dataset that where we can get the ID of the episode the name correspond to the ID of the episode, and the ID for the TV-series is called "parentTconst". These errors were identified through the exploration of the datasets.

   * Transcipt Dataset: 
   
      +  The main obstacle of the dialogue dataset is that not all the HTML files share the same format. In our first, we were not able to extract the dialogues of 15 episodes. We have overcome this difficulty by incorporating special cases in our scraping code that took into account the special cases that we have detected.
      
      + The second difficulty that we have experienced in the dialogue dataset is the cleaning of the dialogues itself. We tried to standardize as most as possible the content of the dialogues, by identifying different names for the same character, common typos and regular expressions that could hinder our analysis.

## **III. Data transformation**

The objective of the data transformation stage was to extract the information from our 2 data sources, IMDb and Github repository, and built a unique Metabase that constitued our main dataframe to carry out our quantitative analyisis of **Friends** and generate suitable graphics. To achive our objetive we have followed the next steps:

   * Step 1: 
      + Step 1.1: Scrap html documents of the transcripts, using the libraries [`rvest`](https://www.rdocumentation.org/packages/rvest/versions/0.3.5){target="_blank"} and [`robotstxt`](https://www.rdocumentation.org/packages/robotstxt/versions/0.6.2){target="_blank"}, and create a data frame with the name of the character, dialogue line, scene, episode and count the number of words.
```{r include=FALSE}

url <- "https://fangj.github.io/friends/"
#paths_allowed(url)


html <- read_html(url)
episodes_list <- html %>% html_nodes("li")  %>% html_nodes("a")
link <- episodes_list %>% html_attr("href")
long_name <- episodes_list %>% html_text
df <- data.frame(long_name,link) %>%
  mutate(numbers = str_extract(long_name,"(\\d+-\\d+)|\\d+") ) %>%
  mutate(season=ifelse(str_length(str_match(numbers,"\\d+"))==3, substr(numbers,0,1),substr(numbers,0,2) ) ) %>%
  mutate(number=str_replace(str_replace(numbers,season,""), paste("-",season,sep = "")  ,"-") ) %>%
  mutate(name = trimws(str_replace(long_name,"(\\d+-\\d+)|\\d+","")) )%>%
  mutate(link = paste(url,link,sep="")) %>%
  mutate(id=paste(season,":",number))

df <- df[colnames(df)!="numbers"]

read_dialogues <- function(link,ep_id) {
  html <- read_html(link)
  #### Special case for episode 9 : 15
  if (ep_id == "9 : 15"){
    df_1 <- read.csv("C:/Users/mamun/Documents/EDVA Fall 2019/Info Project/episode_9_15.csv")
    df_2 <- df_1 %>%
      mutate(rows2= trimws(str_replace(rows,"\\(([^\\)]+)\\)","") )) %>%
      mutate(is_scene = ifelse(substr(rows2,0,1)=="[",1,0))
  }else{
    if(length(html %>% html_nodes("p,font p"))<10 ){
      rows <- html %>% html_nodes("font[size='3']") %>% 
        gsub(pattern = '<.*?>', replacement = "|")
      if(length(rows)==0){
        rows <- html %>%
          gsub(pattern = '<br\\s*/?>', replacement = "|")
      }
      
      df_1 <-data.frame()
      for (i in 1:length(rows)) {
        rows_2 <- strsplit(rows[i],split="[|]")
        df_temp <- data.frame(rows_2)
        names(df_temp) <- c("rows")
        df_1 <- rbind(df_1, df_temp)
      }
      
      df_2 <- df_1 %>%    
        mutate(rows2= trimws(rows)) %>%
        mutate(rows2 = str_replace_all(rows2, "[\n]" , " ")) %>%
        mutate(rows2 = str_replace_all(rows2, "<b>" , "")) %>%
        mutate(rows2 = str_replace_all(rows2, "</b>" , "")) %>%
        mutate(is_scene = ifelse(substr(rows2,0,1)=="[" | substr(rows2,0,1)=="(",1,0)) %>%
        mutate(rows2= trimws(ifelse(is_scene==1,rows2,str_replace(rows2,"\\[([^\\)]+)\\]",""))   ))
      
    }else{
      rows <- html %>% html_nodes("p,font p") %>% html_text()  
      df_1 <- data.frame(rows)
      df_2 <- df_1 %>%
        mutate(rows2= trimws(str_replace(rows,"\\(([^\\)]+)\\)","") )) %>%
        mutate(is_scene = ifelse(substr(rows2,0,1)=="[",1,0))
    } 
  }
  
  df_2$split <- str_locate(df_2[,"rows2"],  "\\w+:|\\w+.\\w+:|\\w+.\\s\\w+:" )[,2]
  
  df_3 <- df_2 %>%
    mutate(character=ifelse(is.na(split),"",toupper(trimws(substr(rows2,0,split - 1 ))))) %>%
    mutate(line=ifelse(is.na(split),"",trimws(substr(rows2,split + 1,1000000)))) %>%
    mutate(episode_id = ep_id) %>%
    mutate(line=trimws(str_remove(line,"\\(([^\\)]+)\\)")) ) %>%
    filter(character != "WRITTEN BY")
  
  
  df_4 <- data.frame(df_3, scene=cumsum(df_3$is_scene))
  
  df_5 <- df_4 %>%
    filter(rows2!="",!is_scene,!is.na(character),character!="")
  
  df_5$line_num <- seq.int(nrow(df_5))
  df_6<- df_5 %>%
    dplyr::select(c("episode_id","line_num","scene",'character','line'))
  
  return(df_6)
}

df2<-read_dialogues(df[203,"link"],df[203,"id"])



dialogues <- data.frame()
for (i in 1:nrow(df)) {
  n <- read_dialogues(df[i,"link"],df[i,"id"])
  #print(i)
  #print(nrow(n))
  dialogues <- rbind(dialogues, n)
  
}


dialogues <- dialogues %>%
  mutate(words = sapply(strsplit(line, " "), length))


unique(filter(dialogues, grepl("-",episode_id))$episode_id)


dialogues <- dialogues %>%
  mutate(episode_id = if_else(episode_id == "2 : 12-13" & scene < 36,"2 : 12",episode_id)) %>%
  mutate(episode_id = if_else(episode_id == "2 : 12-13" & scene >= 36,"2 : 13",episode_id)) %>%
  mutate(episode_id = if_else(episode_id == "6 : 15-16" & scene < 15,"6 : 15",episode_id)) %>%
  mutate(episode_id = if_else(episode_id == "6 : 15-16" & scene >= 15,"6 : 16",episode_id)) %>%
  mutate(episode_id = if_else(episode_id == "9 : 23-24" & scene < 16,"9 : 23",episode_id)) %>%
  mutate(episode_id = if_else(episode_id == "9 : 23-24" & scene >= 16,"9 : 24",episode_id)) %>%
  mutate(episode_id = if_else(episode_id == "10 : 17-18" ,"10 : 17",episode_id))


dialogues <- dialogues %>%
  mutate(season =  trimws(str_extract(episode_id,".+\\:"))) %>%
  mutate(episode =trimws(str_extract(episode_id,"\\:.+")))

dialogues <- dialogues %>%
  mutate(season =  as.integer( trimws(str_sub(season, end=-2)))) %>%
  mutate(episode = as.integer(trimws(str_sub(episode, start=2))))



dialogues <- dialogues %>%
  mutate(character = if_else(character == "RACH" ,"RACHEL", character)) %>%
  mutate(character = if_else(character == "MNCA" ,"MONICA", character)) %>%
  mutate(character = if_else(character == "CHAN" ,"CHANDLER", character)) %>%
  mutate(character = if_else(character == "PHOE" ,"PHOEBE", character)) 


## added this line to eliminate auxiliary data bases and objects that are not going to be useful

rm(df2, episodes_list, df, n, html, i, link, long_name, url)

dialogues=filter(dialogues, dialogues$character!="TELEPLAY BY" &  
                    dialogues$character!="PART I WRITTEN BY" &  
                    dialogues$character!="{TRANSCRIBER'S NOTE" &
                    dialogues$character!="STORY BY" &
                    dialogues$character!="{TRANSCIBER<U+0092>S NOTE"&
                    dialogues$character!="TRANSCRIBED BY"&
                    dialogues$character!="STAGE MANAGER"&
                    dialogues$character!="DIRECTED BY"&
                    dialogues$character!='<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 TRANSITIONAL//EN" "HTTP'&
                    dialogues$character!="{NOTE"&
                    dialogues$character!="CREDITS [SCENE"&
                    dialogues$character!="ORIGINALLY WRITTEN BY MICHAEL CURTIS AND GREG MALINS"&
                    dialogues$character!="(THE COMMERCIAL"&
                    dialogues$character!="AIRED"&
                    dialogues$character!="{TRANSCIBER\u0092S NOTE"&
                    dialogues$character!="{TRANSCRIBER\u0092S NOTE")

```
      
```{r echo=FALSE}
str(dialogues)
```  
      
      
      + Step 1.2: Extract, decompress and save as dataframes. Tha 3 tables of IMDb that we have used in our analysis are those that allowed us to extract the information related to the rating of each episode.
      
```{r include=FALSE}
load(file = "C:/Users/mamun/Documents/EDVA Fall 2019/Info Project/dfratings.RData")
load(file = "C:/Users/mamun/Documents/EDVA Fall 2019/Info Project/dfTitle.RData")
load(file = "C:/Users/mamun/Documents/EDVA Fall 2019/Info Project/dfepisode.RData")
```
*title.ratings.tsv.gz*
```{r echo=FALSE}
str(dfratings)
```         
*name.basics.tsv.gz*
```{r echo=FALSE}
str(dfTitle)
```
*title.episode.tsv.gz*
```{r echo=FALSE}
str(dfepisode)
```

   * Step 2: Join the 3 data frames of IMDb to create an intermediate dataframe.Notice that this dat frame will contain the average rating of IMDb per episode. Furthermore, we created a suitable key to join this dataframe with the dataframe of that contain the dialogues.

```{r include=FALSE}

IMDbFriends=filter(dfTitle, primaryTitle=='Friends' & startYear=='1994' & endYear=='2004') # Friends vector identifier in IMDBd

# rename columns to avoid duplicate column name duplicated with dfratings and dfepisode

colnames(IMDbFriends)=c('parentTconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genre')

# Get the ID from for each episode

IMDbEpisodes=inner_join(IMDbFriends[,1:3],dfepisode, by=c("parentTconst"="parentTconst"))

# Get the rating for each episode

IMDbRatings=left_join(IMDbEpisodes,dfratings, by=c("tconst"="tconst"))

rm(dfratings,dfTitle, dfepisode)


#Create key to join IMDb information with dialogues data base

IMDbRatings=IMDbRatings %>%
  mutate(episode_id=paste(as.character(IMDbRatings$seasonNumber), ":",
                          ifelse(as.numeric(as.character(IMDbRatings$episodeNumber))>=10,
                                 as.character(IMDbRatings$episodeNumber),
                                 paste0(as.character("0"),as.character(IMDbRatings$episodeNumber)))))
```

```{r echo=FALSE}
str(IMDbRatings)
```
   
   * Step 3: With a Left-Join we created the Metabase that had as primary dataframe the dialogues and as secondary dataframe the ratimgs from IMDb.
   
```{r include=FALSE}
dialogues=inner_join(dialogues,IMDbRatings[,c(1,4,7,8,9)], by= c("episode_id"="episode_id"))
```     

```{r echo=FALSE}
str(dialogues)
```

## **V. Results**

### ***i) Labeling the main and secondary characters and its participation***

* The first big question that we want to explore is the categorization by importance by the participation of the characters? As we have previously mentioned this question seems to be naive, however, if we assume that we do not have any previous knowledge of the sitcom and considering that over the ten seasons more than 800 characters appeared in the show, the analysis is far from being a naive exercise.

To answer this question we have used the unsupervised Machine Learning technique of Kmean. Its objective s to label the data based on certain characteristics, in this case, we used the number of words, lines, and scenes. To accomplish this task we have used the libraries  [`cluster`](https://www.rdocumentation.org/packages/cluster/versions/2.1.0){target="_blank"}, and [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2){target="_blank"}. Moreover, we have established established a priori the desired number of labels tha we wanted for our data, for practicity terms we decides to set the size of the groups or kmeans.

```{r include=FALSE}


DF_Character_Episode=dialogues%>%
  group_by(episode, season, scene, character, episode_id)%>%
  summarise(num_lines= n(), totwords=sum(words,na.rm = T), avgRating=mean(averageRating,na.rm = T) )%>%
  arrange(scene ,-num_lines, -totwords)%>%
  group_by(character, episode, season, episode_id)%>%
  summarise(Total_scene= n(), Total_lines=sum(num_lines,na.rm = T),Tot_words=sum(totwords,na.rm = T),Avg_Rating=mean(avgRating,na.rm = T) )%>%
  arrange(-Total_scene ,-Total_lines, -Tot_words)

DF_Character=dialogues%>%
  group_by(episode, season, scene, character, episode_id)%>%
  summarise(num_lines= n(), totwords=sum(words,na.rm = T))%>%
  arrange(scene ,-num_lines, -totwords)%>%
  group_by(character)%>%
  summarise(Total_scene= n(), Total_lines=sum(num_lines,na.rm = T),Total_words=sum(totwords,na.rm = T))%>%
  arrange(-Total_scene ,-Total_lines, -Total_words)


set.seed(100)
ClusterCharacter=kmeans(DF_Character[,2:4], 3, nstart = 100)
vcluster=seq.int(1,3,1)
Centers=arrange(data.frame(ClusterCharacter$centers, vcluster), -Total_scene, -Total_lines, -Total_words)
DF_kmeans=cbind(DF_Character, as.data.frame(ClusterCharacter$cluster))
colnames(DF_kmeans)[5]=c('Cluster')

DF_kmeans=DF_kmeans%>%mutate(Group=ifelse(DF_kmeans$Cluster==Centers[1,4],"Main",ifelse(DF_kmeans$Cluster==Centers[2,4],"Secondary", "Others")))


Gclusters1=filter(DF_kmeans, Cluster== Centers[1,4] | Cluster== Centers[2,4] )%>%
  ggplot(aes(Total_scene, Total_lines, color=factor(Group), label= character)) +geom_point(alpha=0.7, stroke=0.5, size=3)+
  geom_text_repel(size=3)+
  scale_x_log10() +
  scale_y_log10()

x=filter(DF_kmeans, Cluster== Centers[1,4] | Cluster== Centers[2,4] )

```

From the kmean analysis obtain the following separation of characters:
  + *Main Characters*: As expected Rachel, Monica, Phoebe, Joey, Chandler and Ross constitue one group tha has that on average has 1,680 scenes, 8469 lines and 	87,498 words.
  + *Secondary Characters*: This group is composed by 33 characters, most of them are recurrent characters and guest stars. The average character in this group has on average 35 scenes, 133 lines and 	1,228 words.
  
  + *Other Characters*: composed for those characters that are incidental or did not have a relevant importance in the sitcom. The average character in this group has 2 scenes, 7 lines and 64 words.
  
Centers: 

```{r echo=FALSE}
Centers
```


```{r echo=FALSE}
Gclusters1
```

Interactivity Scatter Plot


```{r include=FALSE}
ui <- fluidPage(
  fluidRow(
    
    column(width = 10, class = "well",
           h4("Brush the Points and Get a Zoom and get the data"),
           fluidRow(
             column(width = 6,
                    plotOutput("plot2", height = 400,
                               brush = brushOpts(
                                 id = "plot2_brush",
                                 resetOnNew = TRUE
                               )
                    )
             ),
             column(width = 6,
                    plotOutput("plot3", height = 400)
             )
           )
    )
    
  ),
  
  verbatimTextOutput("info"),
  
)

server <- function(input, output) {
  
  
  # -------------------------------------------------------------------
  # Linked plots (middle and right)
  ranges2 <- reactiveValues(x = NULL, y = NULL)
  
  output$plot2 <- renderPlot({
    filter(DF_kmeans, (Cluster== Centers[1,4] | Cluster== Centers[2,4] | Cluster== Centers[3,4])& Total_scene>=10)%>%
      ggplot(aes(Total_scene, Total_lines, color=factor(Group))) +geom_point(alpha=0.7, stroke=0.5, size=3)
  })
  
  output$plot3 <- renderPlot({
    filter(DF_kmeans, (Cluster== Centers[1,4] | Cluster== Centers[2,4] | Cluster== Centers[3,4])& Total_scene>=10)%>%
      ggplot(aes(Total_scene, Total_lines, color=factor(Group))) +geom_point(alpha=0.7, stroke=0.5, size=3) +
      coord_cartesian(xlim = ranges2$x, ylim = ranges2$y, expand = FALSE)
  })
  
  # When a double-click happens, check if there's a brush on the plot.
  # If so, zoom to the brush bounds; if not, reset the zoom.
  observe({
    brush <- input$plot2_brush
    if (!is.null(brush)) {
      ranges2$x <- c(brush$xmin, brush$xmax)
      ranges2$y <- c(brush$ymin, brush$ymax)
      
    } else {
      ranges2$x <- NULL
      ranges2$y <- NULL
    }
  })
  
  output$info <- renderPrint({
    brushedPoints( filter(DF_kmeans, (Cluster== Centers[1,4] | Cluster== Centers[2,4] | Cluster== Centers[3,4])& Total_scene>=10), input$plot2_brush)
  })
  
  
  
}

shinyApp(ui = ui, server = server)

```



```{r echo=FALSE, fig.width=7, fig.height=40}
shinyApp(ui = ui, server = server)
``` 

### ***ii) Unraveling the character interactions ***

For the Network analysis, a special data structure is required. We established a definition of interaction between characters when they share the same scene. We must mention that the original data structure of the dialogues does not permit us to identify the exact interaction of the characters in the scene. Hence, we have assumed that all the characters that appeared in every scene interacted between them. Moreover, we have assumed that the interactions between the characters will be represented by an adjacency matrix where we can observe the number of interactions that each character has with the others.

With the library [`igraph`](https://www.rdocumentation.org/packages/igraph/versions/1.2.4.2){target="_blank"}) we were able to create the adjecency matrix of the characters and quantify the interactions among the 869 characters. 



```{r include=FALSE}


num_episode=as.data.frame(dialogues$episode_id[!duplicated(dialogues$episode_id)])
colnames(num_episode)[1]=c('episode_id')
Interaction=data.frame(from=character(), to=character(), episode=character(), scene=numeric(), stringsAsFactors=TRUE)


for (i in 1:nrow(num_episode)) 
{ 
  
  BD_episode=filter(dialogues,dialogues$episode_id==num_episode[i,])
  
  num_scene=as.data.frame(BD_episode$scene[!duplicated(BD_episode$scene)])
  
  colnames(num_scene)[1]=c('scene')
  
  for (j in 1:nrow(num_scene)) 
  {
    
    BD_scene=filter(BD_episode,BD_episode$scene==num_scene[j,])
    
    num_character=as.data.frame(BD_scene$character[!duplicated(BD_scene$character)])
    
    colnames(num_character)[1]=c('character')
    
    from=num_character %>% slice(rep(1:n(), each = n()))
    
    to=do.call("rbind", replicate(nrow(num_character), num_character, simplify = FALSE))
    
    episode_i=do.call("rbind", replicate(nrow(to), as.character(num_episode[i,]), simplify = FALSE))
    
    scene_i=do.call("rbind", replicate(nrow(to), num_scene[j,], simplify = FALSE))
    
    Interaction_i=data.frame(from, to, episode_i, scene_i )
    
    colnames(Interaction_i)=c('from','to','episode', 'scene')
    
    Interaction_i=subset(Interaction_i, from!=to)
    
    Interaction=rbind(Interaction,Interaction_i)
    
  }
  
  rm(Interaction_i,scene_i,episode_i,from, to, num_character, num_scene, BD_episode, BD_scene)
  
}

### 

G_Inter=graph_from_data_frame(Interaction, directed = FALSE, vertices = NULL)

Mat_Inter=as_adjacency_matrix(G_Inter, sparse=FALSE)

Mat_Inter=Mat_Inter/2

Mat_main=Mat_Inter[x[,1],x[,1]]

G_Inter_main=graph_from_adjacency_matrix(Mat_main, mode='undirected', weighted = TRUE)


```


```{r echo=FALSE}
plot(G_Inter_main, layout=layout_in_circle, vertex.label=3)
```


Interactivity Networks


```{r include=FALSE}


data=toVisNetworkData(G_Inter_main)

nodes = inner_join(data$nodes,DF_kmeans[,c(1,5,6)], by=c("id"="character"))
colnames(nodes)[c(4)]=c('group')

edges=data$edges
edges$value=edges$weight

GNet=visNetwork(nodes, edges, height = "1000px", width="1000px") %>%
  visIgraphLayout(layout = "layout_in_circle")%>%
  visNodes(size = 10) %>%
  visOptions(selectedBy = "group",
             highlightNearest = list(enabled = TRUE, algorithm = "hierarchical", degree = 1),
             nodesIdSelection = TRUE)%>%
  visInteraction(hideEdgesOnDrag = TRUE)%>%
  visEdges(color = list(highlight = "blue", hover = "red"))%>%
  visGroups(groupname = "Main", color = "red", shape = "square", 
            shadow = list(enabled = TRUE))%>%
  visGroups(groupname = "Secondary", color = "blue", shape = "dot")

```



```{r echo=FALSE}
GNet
```


### ***iii) Extracting the main topics***

### ***iv) Rating contribution per character***


## **VI. Interactive component**




## **VII. Conclusion**